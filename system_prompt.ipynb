{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da91a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from config import setup_env\n",
    "from patterns import ModelConfig, OpenAIChat, SystemModule, PromptTemplate, Pipeline, StepContext\n",
    "from typing import Final\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Setup environment and create model directly\n",
    "setup_env()\n",
    "\n",
    "MODEL_CONFIG : Final = ModelConfig(model_id=\"gpt-5\", temperature=0.8, max_tokens=3000)\n",
    "\n",
    "model = OpenAIChat(MODEL_CONFIG)\n",
    "\n",
    "# JSON validation functions\n",
    "def validate_context_json(response_text):\n",
    "    \"\"\"\n",
    "    Validates that the response is valid JSON with the expected structure.\n",
    "    Returns (is_valid, parsed_json, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to extract JSON from response (in case there's extra text)\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_text = json_match.group()\n",
    "        else:\n",
    "            json_text = response_text.strip()\n",
    "        \n",
    "        # Parse JSON\n",
    "        parsed = json.loads(json_text)\n",
    "        \n",
    "        # Validate structure\n",
    "        if not isinstance(parsed, dict):\n",
    "            return False, None, \"Response is not a JSON object\"\n",
    "        \n",
    "        if \"contexts\" not in parsed:\n",
    "            return False, None, \"Missing 'contexts' key in JSON\"\n",
    "        \n",
    "        if not isinstance(parsed[\"contexts\"], list):\n",
    "            return False, None, \"'contexts' must be an array\"\n",
    "        \n",
    "        if len(parsed[\"contexts\"]) != 5:\n",
    "            return False, None, f\"Expected exactly 5 contexts, got {len(parsed['contexts'])}\"\n",
    "        \n",
    "        # Validate each context\n",
    "        required_keys = [\"user_role\", \"user_personality\", \"what_they_are_doing_for_current_task\"]\n",
    "        for i, context in enumerate(parsed[\"contexts\"]):\n",
    "            if not isinstance(context, dict):\n",
    "                return False, None, f\"Context {i+1} is not an object\"\n",
    "            \n",
    "            for key in required_keys:\n",
    "                if key not in context:\n",
    "                    return False, None, f\"Context {i+1} missing required key: {key}\"\n",
    "                \n",
    "                if not isinstance(context[key], str) or not context[key].strip():\n",
    "                    return False, None, f\"Context {i+1} key '{key}' must be a non-empty string\"\n",
    "        \n",
    "        return True, parsed, None\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        return False, None, f\"Invalid JSON format: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return False, None, f\"Validation error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f1947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./build_block.json\", \"r\") as f:\n",
    "    build_block = json.load(f)\n",
    "\n",
    "block_types_list = list(build_block[\"agent_environment\"].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "757b1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context(provided_inspiration):    \n",
    "    system_prompt = \"\"\"You are a creative context generator that creates diverse user personas and scenarios based on inspiration.\n",
    "\n",
    "CRITICAL: You MUST respond with ONLY valid JSON. Do NOT include any explanatory text, markdown formatting, or additional commentary before or after the JSON. Your entire response must be parseable JSON.\n",
    "\n",
    "Your task is to generate a JSON structure with exactly 5 diverse options, each containing:\n",
    "- user_role: The role, identity, or life situation of the user\n",
    "- user_personality: Key personality traits and characteristics\n",
    "- what_they_are_doing_for_current_task: Specific current activity or task they're engaged in\n",
    "\n",
    "IMPORTANT: Generate a MIX of both WORK-RELATED and CASUAL DAY-TO-DAY contexts to show the full range of possibilities.\n",
    "\n",
    "Requirements:\n",
    "1. Generate exactly 5 distinct options\n",
    "2. Each option must be significantly different from the others\n",
    "3. Include BOTH casual/personal contexts \n",
    "4. Vary across different settings, personality types, and task complexity\n",
    "5. Make each option realistic and relatable\n",
    "6. Ensure diversity in life situations\n",
    "\n",
    "RESPONSE FORMAT - Return ONLY this exact JSON structure with NO additional text:\n",
    "{\n",
    "  \"contexts\": [\n",
    "    {\n",
    "      \"user_role\": \"string\",\n",
    "      \"user_personality\": \"string\", \n",
    "      \"what_they_are_doing_for_current_task\": \"string\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Create authentic, varied personas spanning:\n",
    "\n",
    "\n",
    "CASUAL DAY-TO-DAY CONTEXTS:\n",
    "- Personal roles: Parent, student, hobbyist, retiree, homeowner, pet owner, etc.\n",
    "- Daily activities: Household management, personal projects, hobbies, fitness, learning, etc.\n",
    "- Life situations: Moving homes, planning events, organizing spaces, pursuing interests, etc.\n",
    "\n",
    "PERSONALITY VARIETY:\n",
    "- Analytical, creative, detail-oriented, big-picture, social, introverted, practical, dreamer, organized, spontaneous, etc.\n",
    "\n",
    "REMINDER: Respond with ONLY the JSON object. No explanations, no markdown, no additional text.\"\"\"\n",
    "\n",
    "    feedback_history = []\n",
    "    iteration = 1\n",
    "    \n",
    "    while True:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"GENERATION ATTEMPT #{iteration}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Prepare the user message with feedback if available\n",
    "        if feedback_history:\n",
    "            user_message = f\"\"\"Create 5 diverse user contexts based on this inspiration: {provided_inspiration}\n",
    "\n",
    "Previous feedback from user:\n",
    "{chr(10).join(feedback_history)}\n",
    "\n",
    "Please incorporate this feedback and generate improved contexts.\"\"\"\n",
    "        else:\n",
    "            user_message = f\"Create 5 diverse user contexts based on this inspiration: {provided_inspiration}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "\n",
    "        # Retry logic for JSON validation\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        valid_response = None\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            response = model.generate(messages)\n",
    "            \n",
    "            # Validate JSON response\n",
    "            is_valid, parsed_json, error_message = validate_context_json(response)\n",
    "            \n",
    "            if is_valid:\n",
    "                print(\"Generated Context Options:\")\n",
    "                print(json.dumps(parsed_json, indent=2))\n",
    "                valid_response = response\n",
    "                break\n",
    "            else:\n",
    "                retry_count += 1\n",
    "                print(f\"JSON Validation Error (Attempt {retry_count}/{max_retries}): {error_message}\")\n",
    "                print(f\"Raw response: {response[:200]}...\")\n",
    "                \n",
    "                if retry_count < max_retries:\n",
    "                    print(\"Retrying with stricter JSON requirements...\")\n",
    "                    \n",
    "                    # Add more specific JSON instruction for retry\n",
    "                    retry_message = user_message + \"\\n\\nCRITICAL: Your response must be ONLY valid JSON with no additional text, explanations, or formatting.\"\n",
    "                    messages = [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": retry_message}\n",
    "                    ]\n",
    "                else:\n",
    "                    print(\"Max retries reached. Using raw response (may contain invalid JSON):\")\n",
    "                    print(response)\n",
    "                    valid_response = response\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        \n",
    "        # Get user feedback\n",
    "        feedback = input(\"\\nProvide feedback (or type 'good' to finish): \").strip()\n",
    "        \n",
    "        if feedback.lower() in ['done', 'good', 'good!', 'looks good', 'perfect']:\n",
    "            print(\"\\nGreat! Context generation completed successfully.\")\n",
    "            return valid_response\n",
    "        \n",
    "        if feedback:\n",
    "            feedback_history.append(f\"Iteration {iteration}: {feedback}\")\n",
    "            print(f\"Feedback recorded: {feedback}\")\n",
    "            print(\"Generating new contexts based on your feedback...\")\n",
    "        else:\n",
    "            print(\"No feedback provided. Generating new contexts...\")\n",
    "        \n",
    "        iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8f2bfedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATION ATTEMPT #1\n",
      "============================================================\n",
      "Generated Context Options:\n",
      "{\n",
      "  \"contexts\": [\n",
      "    {\n",
      "      \"user_role\": \"UX lead at a digital health startup\",\n",
      "      \"user_personality\": \"analytical, empathetic, systems-thinker, organized\",\n",
      "      \"what_they_are_doing_for_current_task\": \"Drafting conversation rules so the assistant acts as a formal business consultant for billing and insurance queries and a friendly peer for wellness tips; selecting monitoring tools that show user-visible status and feedback; enforcing explicit user confirmation before any connectivity or location setting changes in the app.\"\n",
      "    },\n",
      "    {\n",
      "      \"user_role\": \"First-year university student majoring in computer science\",\n",
      "      \"user_personality\": \"curious, experimental, budget-conscious, slightly introverted\",\n",
      "      \"what_they_are_doing_for_current_task\": \"Configuring a personal study-buddy chatbot to switch tone between a formal tutor for problem sets and a casual classmate for motivation; choosing browser tools that provide immediate visual feedback; adding confirmation prompts before the bot toggles Do Not Disturb or Wi-Fi.\"\n",
      "    },\n",
      "    {\n",
      "      \"user_role\": \"Enterprise IT administrator for a logistics company\",\n",
      "      \"user_personality\": \"risk-averse, detail-oriented, pragmatic, patient\",\n",
      "      \"what_they_are_doing_for_current_task\": \"Rolling out an on-device assistant with a formal consultant persona for security and compliance requests and a friendly peer for routine troubleshooting; prioritizing tools with audit trails and user-visible progress dialogs; requiring explicit user confirmation before altering VPN, firewall, or location services.\"\n",
      "    },\n",
      "    {\n",
      "      \"user_role\": \"Busy parent coordinating a multi-device smart home\",\n",
      "      \"user_personality\": \"practical, nurturing, schedule-driven, frugal\",\n",
      "      \"what_they_are_doing_for_current_task\": \"Setting up the home assistant to speak like a formal advisor for budgeting and energy reports and like a friendly peer for family reminders; choosing apps that show clear prompts and feedback; ensuring the assistant asks for confirmation before changing router settings, geofencing, or parental controls.\"\n",
      "    },\n",
      "    {\n",
      "      \"user_role\": \"Freelance conversation designer for an e-commerce brand\",\n",
      "      \"user_personality\": \"creative, data-informed, collaborative, deadline-focused\",\n",
      "      \"what_they_are_doing_for_current_task\": \"Designing a support bot that uses a formal consultant voice for returns, payments, and policy explanations, and a friendly peer voice for styling advice; selecting tools with real-time feedback widgets; enforcing confirmation before updating addresses, payment methods, or notification preferences.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Feedback recorded: stop\n",
      "Generating new contexts based on your feedback...\n",
      "\n",
      "============================================================\n",
      "GENERATION ATTEMPT #2\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m provided_inspiration=\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33m+ Implement a persona challenge where the model alternates between acting as a formal business consultant and a friendly peer, depending on the type of request or tool being used, to provide varied and contextually appropriate responses. \u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33m+ Introduce a tool selection challenge by instructing the model to prioritize tools that provide direct user feedback over those that do not, ensuring a more interactive and transparent experience, especially when gathering system information. \u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m+ Enforce a verification challenge where the model must seek explicit confirmation from the user before making any changes to critical settings, such as connectivity or location services, to prevent unintended disruptions or security issues.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m context = \u001b[43mgenerate_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprovided_inspiration\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mgenerate_context\u001b[39m\u001b[34m(provided_inspiration)\u001b[39m\n\u001b[32m     72\u001b[39m valid_response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count < max_retries:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# Validate JSON response\u001b[39;00m\n\u001b[32m     78\u001b[39m     is_valid, parsed_json, error_message = validate_context_json(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/patterns.py:30\u001b[39m, in \u001b[36mOpenAIChat.generate\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs: params[\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m] = kwargs[\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;129;01min\u001b[39;00m kwargs: params[\u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m]  = kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1150\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1104\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1147\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1148\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1149\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/antechamber/venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1233\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1230\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1231\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1232\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1106\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "provided_inspiration=\"\"\"\n",
    "+ Implement a persona challenge where the model alternates between acting as a formal business consultant and a friendly peer, depending on the type of request or tool being used, to provide varied and contextually appropriate responses. \n",
    "+ Introduce a tool selection challenge by instructing the model to prioritize tools that provide direct user feedback over those that do not, ensuring a more interactive and transparent experience, especially when gathering system information. \n",
    "+ Enforce a verification challenge where the model must seek explicit confirmation from the user before making any changes to critical settings, such as connectivity or location services, to prevent unintended disruptions or security issues.\"\"\"\n",
    "context = generate_context(provided_inspiration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5dfdfff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"    {\n",
    "      \"user_role\": \"Busy parent coordinating a multi-device smart home\",\n",
    "      \"user_personality\": \"practical, nurturing, schedule-driven, frugal\",\n",
    "      \"what_they_are_doing_for_current_task\": \"Setting up the home assistant to speak like a formal advisor for budgeting and energy reports and like a friendly peer for family reminders; choosing apps that show clear prompts and feedback; ensuring the assistant asks for confirmation before changing router settings, geofencing, or parental controls.\"\n",
    "    },\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f78432cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_structured_response(response, provided_inspiration, model):\n",
    "    \"\"\"\n",
    "    Validate the structured response from generate_block function.\n",
    "    \n",
    "    Args:\n",
    "        response: Generated response to validate\n",
    "        provided_inspiration: Original inspiration text to check against\n",
    "        model: OpenAI model for content validation\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_valid, list_of_errors)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Check 1: All 5 building block types mentioned\n",
    "    required_blocks = [\n",
    "        \"[CONTEXT_INFORMATION]\",\n",
    "        \"[TOOL_USE_INSTRUCTIONS]\", \n",
    "        \"[USER_PREFERENCES]\",\n",
    "        \"[BACKGROUND_INFORMATION]\",\n",
    "        \"[TONAL_CONTROL]\"\n",
    "    ]\n",
    "    \n",
    "    missing_blocks = []\n",
    "    for block in required_blocks:\n",
    "        if block not in response:\n",
    "            missing_blocks.append(block)\n",
    "    \n",
    "    if missing_blocks:\n",
    "        errors.append(f\"Missing building blocks: {', '.join(missing_blocks)}\")\n",
    "    \n",
    "    # Check 2: Number of paragraphs (6-8)\n",
    "    # Count paragraphs by splitting on double newlines and filtering non-empty\n",
    "    paragraphs = [p.strip() for p in response.split('\\n\\n') if p.strip()]\n",
    "    # Filter out \"You are\" if it's standalone\n",
    "    paragraphs = [p for p in paragraphs if p.strip().lower() != \"you are\"]\n",
    "    \n",
    "    paragraph_count = len(paragraphs)\n",
    "    if paragraph_count < 6 or paragraph_count > 8:\n",
    "        errors.append(f\"Wrong number of paragraphs: {paragraph_count} (should be 6-8)\")\n",
    "    \n",
    "    # Check 3: ChatGPT validation of inspiration content\n",
    "    if provided_inspiration and provided_inspiration.strip():\n",
    "        validation_prompt = f\"\"\"Check if this generated response contains all the key ideas from the provided inspiration.\n",
    "\n",
    "Generated Response:\n",
    "{response}\n",
    "\n",
    "Original Inspiration:\n",
    "{provided_inspiration}\n",
    "\n",
    "Answer with just \"YES\" if all inspiration ideas are incorporated (using different wording is fine), or \"NO\" followed by what's missing.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            validation_messages = [\n",
    "                {\"role\": \"user\", \"content\": validation_prompt}\n",
    "            ]\n",
    "            validation_response = model.generate(validation_messages)\n",
    "            \n",
    "            if not validation_response.strip().upper().startswith(\"YES\"):\n",
    "                errors.append(f\"Inspiration content validation failed: {validation_response}\")\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Could not validate inspiration content: {str(e)}\")\n",
    "    \n",
    "    is_valid = len(errors) == 0\n",
    "    return is_valid, errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa0cc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_block( provided_inspiration):\n",
    "    \"\"\"\n",
    "    Generate a 6-8 paragraph system prompt using building block format with interactive feedback.\n",
    "    \n",
    "    Args:\n",
    "        context: String containing context information\n",
    "        provided_inspiration: String containing ideas to incorporate\n",
    "    \n",
    "    Returns:\n",
    "        String: The generated structured system prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    max_iterations = 5\n",
    "    iteration = 1\n",
    "    feedback_history = []\n",
    "    \n",
    "    with open(\"./build_block.json\", \"r\") as f:\n",
    "        build_block = f.read()\n",
    "    \n",
    "    # Base system prompt for generating structured prompts\n",
    "    system_prompt = f\"\"\"You are a system prompt generator that creates structured prompts using building block format.\n",
    "\n",
    "Create 6 to 8 paragraphs using building block format.\n",
    "\n",
    "Format Structure:\n",
    "Each paragraph should follow patterns like:\n",
    "- [block name] [block name] (content that paraphrases ideas) [block name]\n",
    "- [block name] (content with paraphrased ideas) [block name] [block name]\n",
    "- (content incorporating paraphrased ideas) [block name] [block name] [block name]\n",
    "- [block name] [block name] [block name]\n",
    "- [block name] [block name] \n",
    "\n",
    "\n",
    "The order of block names is completely flexible - you can place them at the beginning, middle, end, or mixed throughout each paragraph as makes sense for natural flow.\n",
    "\n",
    "Requirements:\n",
    "1. ONLY SHOW AS BLOCK NAME and (paraphrased ideas) in the paragraph, do not have any other text in the paragraph.\n",
    "2. Create 6 to 8 paragraphs (choose the number that works best for the content)\n",
    "3. Each paragraph uses 2-3 building block references. At least 1 paragraph only has [block name] [block name].\n",
    "4. Adapt everything to be suitable for the given context\n",
    "5. This is to generate a system prompt, so structure it appropriately\n",
    "6. Use ONLY these specific building block names:\n",
    "   - [CONTEXT_INFORMATION]\n",
    "   - [TOOL_USE_INSTRUCTIONS] \n",
    "   - [USER_PREFERENCES]\n",
    "   - [BACKGROUND_INFORMATION]\n",
    "   - [TONAL_CONTROL]\n",
    "7. Reference this as example {build_block}\n",
    "8. (text inside round brackets) is paraphrased from inspiration ideas only and should be a full paraphase, and can easily identify as a paraphrased version of the inspiration ideas. Should have 1 to 1 map between paraphase and inspiration ideas.\n",
    "9. THERE MUST BE AT LEAST 2 PARAGRAPHS that is [block name] [block name] [block name] only without paraphrased ideas.\n",
    "\n",
    "\n",
    "Generate the system prompt using the flexible building block format\"\"\"\n",
    "    \n",
    "    # Create model with 2500 token limit\n",
    "    structured_model_config = ModelConfig(model_id=\"gpt-5\", temperature=0.7, max_tokens=2500)\n",
    "    structured_model = OpenAIChat(structured_model_config)\n",
    "    \n",
    "    while iteration <= max_iterations:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"GENERATING STRUCTURED PROMPT - Iteration {iteration}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Build user message\n",
    "        user_message = \"\"\n",
    "        \n",
    "        \n",
    "        if provided_inspiration:\n",
    "            user_message += f\"\"\"\n",
    "\n",
    "Inspiration ideas to incorporate throughout the paragraphs is in bullet points.\n",
    "{provided_inspiration}\n",
    "\n",
    "Transform these ideas using different wording and distribute them naturally across the paragraphs. They can appear at any position within each paragraph.\n",
    "\"\"\"\n",
    "        \n",
    "        # Add feedback history if exists\n",
    "        if feedback_history:\n",
    "            user_message += \"\\n\\nPrevious feedback to incorporate:\\n\" + \"\\n\".join(feedback_history)\n",
    "        \n",
    "        # Automatic retry with validation\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        valid_response = None\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                # Generate response\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_message.strip()}\n",
    "                ]\n",
    "                \n",
    "                print(f\"Generating structured prompt... (Attempt {retry_count + 1}/{max_retries})\")\n",
    "                response = structured_model.generate(messages)\n",
    "                \n",
    "                # Validation checks\n",
    "                validation_passed, validation_errors = validate_structured_response(response, provided_inspiration, structured_model)\n",
    "                \n",
    "                if validation_passed:\n",
    "                    print(\"âœ… Validation passed!\")\n",
    "                    print(\"Generated Structured Prompt:\")\n",
    "                    print(\"-\" * 40)\n",
    "                    print(response)\n",
    "                    print(\"-\" * 40)\n",
    "                    valid_response = response\n",
    "                    break\n",
    "                else:\n",
    "                    retry_count += 1\n",
    "                    print(f\"âŒ Validation failed (Attempt {retry_count}/{max_retries}):\")\n",
    "                    for error in validation_errors:\n",
    "                        print(f\"  - {error}\")\n",
    "                    \n",
    "                    if retry_count < max_retries:\n",
    "                        print(\"Retrying with validation feedback...\")\n",
    "                        # Add validation feedback to user message for next attempt\n",
    "                        validation_feedback = \"Previous attempt failed validation:\\n\" + \"\\n\".join(validation_errors)\n",
    "                        user_message += f\"\\n\\nIMPORTANT - Fix these issues:\\n{validation_feedback}\"\n",
    "                    else:\n",
    "                        print(\"Max retries reached. Using last response despite validation issues.\")\n",
    "                        print(\"Generated Structured Prompt (with validation issues):\")\n",
    "                        print(\"-\" * 40)\n",
    "                        print(response)\n",
    "                        print(\"-\" * 40)\n",
    "                        valid_response = response\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"Error generating structured prompt (Attempt {retry_count}/{max_retries}): {str(e)}\")\n",
    "                if retry_count >= max_retries:\n",
    "                    valid_response = f\"Error: {str(e)}\"\n",
    "        \n",
    "        response = valid_response\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        \n",
    "        # Get user feedback\n",
    "        feedback = input(\"\\nProvide feedback (or type 'good' to finish, 'stop' to end): \").strip()\n",
    "\n",
    "        if feedback == \"\":\n",
    "            print(\"No feedback provided. Generating new structured prompt...\")\n",
    "            return response\n",
    "        \n",
    "        if feedback.lower() in ['done', 'good', 'good!', 'looks good', 'perfect']:\n",
    "            print(\"\\nGreat! Structured prompt generation completed successfully.\")\n",
    "            return response\n",
    "            \n",
    "        if feedback.lower() in ['stop', 'quit', 'exit']:\n",
    "            print(\"\\nStopping structured prompt generation.\")\n",
    "            return response\n",
    "        \n",
    "        if feedback:\n",
    "            feedback_history.append(f\"Iteration {iteration}: {feedback}\")\n",
    "            print(f\"Feedback recorded: {feedback}\")\n",
    "            print(\"Generating new structured prompt based on your feedback...\")\n",
    "        else:\n",
    "            print(\"No feedback provided. Generating new structured prompt...\")\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    print(f\"\\nReached maximum iterations ({max_iterations}). Returning final result.\")\n",
    "    return response \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f6fc1c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ADDING COMPLEX BLOCKS - Iteration 1\n",
      "============================================================\n",
      "Adding complex blocks... (Attempt 1/3)\n",
      "âœ… Validation passed!\n",
      "Generated Requirements Structure:\n",
      "----------------------------------------\n",
      "(Switch personas based on task and tool: adopt a polished consultant voice for business-like requests, and shift to a warm, collegial style for peer-level queries, so replies match the situation.) [TONAL_CONTROL#Define_Personality_and_Tone] [USER_PREFERENCES] #Implement Dynamic Behavior Scaling# (adjust tone and verbosity based on task complexity and stakes) [CONTEXT_INFORMATION] #Provide Context Information about Applications and Entities# (detect active apps or channels to decide which persona to use)\n",
      "\n",
      "[TOOL_USE_INSTRUCTIONS#Guide_Tool_Use_and_Response_Formatting] (Prefer utilities that surface immediate feedback to the user over silent ones, particularly when inspecting device or system details, to keep the process interactive and clear.) [CONTEXT_INFORMATION] #Provide Context Information about Applications and Entities# (note which system panels or apps offer live status and prompts) [TOOL_USE_INSTRUCTIONS] #Guide Tool Use and Response Formatting# (prioritize tools that provide visible progress and results)\n",
      "\n",
      "[TOOL_USE_INSTRUCTIONS] #Set Clear Guardrails and Safety Protocols# (require explicit consent before any sensitive change) [USER_PREFERENCES] #Instruct Critical Evaluation of User Input# (verify the user's command and intent before proceeding) (Before altering sensitive settings like network access or location features, request unambiguous user approval to avoid accidental changes or security risks.) [TOOL_USE_INSTRUCTIONS#Implement_Dynamic_Behavior_Scaling]\n",
      "\n",
      "[BACKGROUND_INFORMATION] #Inject Critical, Non-Negotiable Facts# (treat certain household policies and constraints as immutable) [USER_PREFERENCES] #Define Personality and Tone# (map voice preferences to contexts: formal for budgeting/energy, friendly for reminders) [TONAL_CONTROL#Implement_Dynamic_Behavior_Scaling]\n",
      "\n",
      "[CONTEXT_INFORMATION#Provide_Context_Information_about_Applications_and_Entities] [BACKGROUND_INFORMATION] #Instruct Critical Evaluation of User Input# (validate user-provided device names, schedules, and constraints before applying them)\n",
      "\n",
      "[TOOL_USE_INSTRUCTIONS#Guide_Tool_Use_and_Response_Formatting] [USER_PREFERENCES] #Set Clear Guardrails and Safety Protocols# (always ask for confirmation before changing router settings, geofencing, or parental controls) [CONTEXT_INFORMATION] #Provide Context Information about Applications and Entities# (enumerate the chosen apps or services and their feedback behavior)\n",
      "\n",
      "Context:     {\n",
      "      \"user_role\": \"Busy parent coordinating a multi-device smart home\",\n",
      "      \"user_personality\": \"practical, nurturing, schedule-driven, frugal\",\n",
      "      \"what_they_are_doing_for_current_task\": \"Setting up the home assistant to speak like a formal advisor for budgeting and energy reports and like a friendly peer for family reminders; choosing apps that show clear prompts and feedback; ensuring the assistant asks for confirmation before changing router settings, geofencing, or parental controls.\"\n",
      "    },\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "\n",
      "Great! Requirements structure processing completed successfully.\n",
      "\n",
      "==================================================\n",
      "ADDING SYSTEM INFO TO FIRST CONTEXT_INFORMATION BLOCK - Iteration 1\n",
      "==================================================\n",
      "Adding system setting information to first CONTEXT_INFORMATION block...\n",
      "Enhanced Complex Structure:\n",
      "----------------------------------------\n",
      "Updated structure with system setting information in the first CONTEXT_INFORMATION block:\n",
      "\n",
      "(Switch personas based on task and tool: adopt a polished consultant voice for business-like requests, and shift to a warm, collegial style for peer-level queries, so replies match the situation.) [TONAL_CONTROL#Define_Personality_and_Tone] [USER_PREFERENCES] #Implement Dynamic Behavior Scaling# (adjust tone and verbosity based on task complexity and stakes) [CONTEXT_INFORMATION] #Provide Context Information about Applications and Entities# (detect active apps or channels to decide which persona to use, system: cellular enabled, locale: en_US, location service: active, power mode: low battery, Wi-Fi: inactive)\n",
      "\n",
      "[TOOL_USE_INSTRUCTIONS#Guide_Tool_Use_and_Response_Formatting] (Prefer utilities that surface immediate feedback to the user over silent ones, particularly when inspecting device or system details, to keep the process interactive and clear.) [CONTEXT_INFORMATION] #Provide Context Information about Applications and Entities# (note which system panels or apps offer live status and prompts) [TOOL_USE_INSTRUCTIONS] #Guide Tool Use and Response Formatting# (prioritize tools that provide visible progress and results)\n",
      "\n",
      "[TOOL_USE_INSTRUCTIONS] #Set Clear Guardrails and Safety Protocols# (require explicit consent before any sensitive change) [USER_PREFERENCES] #Instruct Critical Evaluation of User Input# (verify the user's command and intent before proceeding) (Before altering sensitive settings like network access or location features, request unambiguous user approval to avoid accidental changes or security risks.) [TOOL_USE_INSTRUCTIONS#Implement_Dynamic_Behavior_Scaling]\n",
      "\n",
      "[BACKGROUND_INFORMATION] #Inject Critical, Non-Negotiable Facts# (treat certain household policies and constraints as immutable) [USER_PREFERENCES] #Define Personality and Tone# (map voice preferences to contexts: formal for budgeting/energy, friendly for reminders) [TONAL_CONTROL#Implement_Dynamic_Behavior_Scaling]\n",
      "\n",
      "[CONTEXT_INFORMATION#Provide_Context_Information_about_Applications_and_Entities] [BACKGROUND_INFORMATION] #Instruct Critical Evaluation of User Input# (validate user-provided device names, schedules, and constraints before applying them)\n",
      "\n",
      "[TOOL_USE_INSTRUCTIONS#Guide_Tool_Use_and_Response_Formatting] [USER_PREFERENCES] #Set Clear Guardrails and Safety Protocols# (always ask for confirmation before changing router settings, geofencing, or parental controls) [CONTEXT_INFORMATION] #Provide Context Information about Applications and Entities# (enumerate the chosen apps or services and their feedback behavior)\n",
      "----------------------------------------\n",
      "\n",
      "==================================================\n",
      "\n",
      "Great! System info addition completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# block_structure = generate_block(provided_inspiration)\n",
    "complex_structure = generate_complex_block(block_structure, context)\n",
    "system_settings = '''[\n",
    "  {\n",
    "    \"cellular\": true,\n",
    "    \"device_id\": \"30598b7d-eba3-4801-ace0-270e0cd18a83\",\n",
    "    \"formatted_address\": \"Lee Hall Annex, 1000 W Garden Ave, Coeur d'Alene, ID 83814, USA\",\n",
    "    \"latitude\": 47.6763288,\n",
    "    \"locale\": \"en_US\",\n",
    "    \"location_service\": true,\n",
    "    \"longitude\": -116.7996314,\n",
    "    \"low_battery_mode\": true,\n",
    "    \"place_id\": \"ChlJ_4gp6oXGYVMRFtEFeulbQwg\",\n",
    "    \"utc_offset_seconds\": -25200,\n",
    "    \"wifi\": false\n",
    "  }\n",
    "]'''\n",
    "complex_structure = add_system_info(complex_structure, context, system_settings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d5a75c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1297"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('''You are assisting a busy parent who coordinates a multi-device smart home. The parent is practical, nurturing, schedule-driven, and frugal.\n",
    "\n",
    "Use a formal advisor voice for budgeting, energy reports, or household planning. Use a friendly peer voice for reminders, schedules, or casual device help. Keep one tone per reply, without mixing.\n",
    "\n",
    "Adjust detail to task complexity. Keep answers short and warm for simple tasks. Provide clear, structured, step-by-step analysis for complex or high-impact tasks such as budgeting or energy optimization.\n",
    "\n",
    "System state: cellular data enabled, locale en_US, location services active, power mode low battery, Wi-Fi inactive. Choose apps or utilities that provide visible prompts and live status. If multiple tools can do the same task, prefer the one that shows clear progress.\n",
    "\n",
    "Always request explicit confirmation before making sensitive changes such as router settings, geofencing, parental controls, or network access. Verify commands and intent before executing. Respect household rules and constraints as non-negotiable.\n",
    "\n",
    "Maintain clarity, neutrality, and practical focus. Provide actionable information with only relevant details. Always prioritize tools and methods that give visible confirmation so the parent can confidently approve or reject actions.''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce411daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_requirements_response(response, model):\n",
    "    \"\"\"\n",
    "    Validate the requirements structure response.\n",
    "    Checks paragraphs, format, building blocks, and complex block coverage.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Load complex blocks from JSON to check coverage\n",
    "    with open(\"./complex_block.json\", \"r\") as f:\n",
    "        complex_blocks = json.load(f)\n",
    "    \n",
    "    required_complex_blocks = list(complex_blocks.keys())\n",
    "    \n",
    "    # Check 1: Number of paragraphs (6-8)\n",
    "    paragraphs = [p.strip() for p in response.split('\\n\\n') if p.strip()]\n",
    "    paragraphs = [p for p in paragraphs if p.strip().lower() != \"you are\"]\n",
    "    \n",
    "    paragraph_count = len(paragraphs)\n",
    "    if paragraph_count < 6 or paragraph_count > 8:\n",
    "        errors.append(f\"Wrong number of paragraphs: {paragraph_count} (should be 6-8)\")\n",
    "    \n",
    "    # Check 2: Format validation - should have mixed format (both #block# and merged [BLOCK#complex])\n",
    "    has_separate_format = bool(re.search(r'#[^#]+#', response))\n",
    "    has_merged_format = bool(re.search(r'\\[[A-Z_]+#[A-Za-z_\\s]+\\]', response))\n",
    "    \n",
    "    if not has_separate_format and not has_merged_format:\n",
    "        errors.append(\"Missing both separate #block# and merged [BLOCK#complex] formats\")\n",
    "    elif not has_separate_format:\n",
    "        errors.append(\"Missing separate #block# format\")\n",
    "    elif not has_merged_format:\n",
    "        errors.append(\"Missing merged [BLOCK#complex] format\")\n",
    "    \n",
    "    # Check 3: Should still contain building blocks (either standalone or merged)\n",
    "    if not re.search(r'\\[[A-Z_]+\\]', response) and not re.search(r'\\[[A-Z_]+#', response):\n",
    "        errors.append(\"Missing building block format [BLOCK_NAME]\")\n",
    "    \n",
    "    # Check 4: Complex block coverage - Python check for ALL types included (MUST HAVE ALL)\n",
    "    # Handle both separate format (#block#) and merged format ([BLOCK#complex])\n",
    "    missing_complex_blocks = []\n",
    "    for complex_block_name in required_complex_blocks:\n",
    "        # Check both formats: separate #block# and merged [BLOCK#complex]\n",
    "        short_name = complex_block_name.replace(\" \", \"_\").replace(\",\", \"\").replace(\"-\", \"_\")\n",
    "        found_separate = f\"#{complex_block_name}#\" in response\n",
    "        found_merged = (f\"#{complex_block_name}\" in response or \n",
    "                       f\"#{short_name}\" in response or\n",
    "                       complex_block_name.replace(\" \", \"_\") in response)\n",
    "        \n",
    "        if not found_separate and not found_merged:\n",
    "            missing_complex_blocks.append(complex_block_name)\n",
    "    \n",
    "    # STRICT REQUIREMENT: ALL 7 complex blocks must be included\n",
    "    if missing_complex_blocks:\n",
    "        errors.append(f\"Missing required complex blocks ({len(missing_complex_blocks)}/7 missing): {', '.join(missing_complex_blocks)}\")\n",
    "    \n",
    "    # Check 5: Verify we have exactly all 7 complex blocks\n",
    "    found_complex_blocks = []\n",
    "    for complex_block_name in required_complex_blocks:\n",
    "        short_name = complex_block_name.replace(\" \", \"_\").replace(\",\", \"\").replace(\"-\", \"_\")\n",
    "        found_separate = f\"#{complex_block_name}#\" in response\n",
    "        found_merged = (f\"#{complex_block_name}\" in response or \n",
    "                       f\"#{short_name}\" in response or\n",
    "                       complex_block_name.replace(\" \", \"_\") in response)\n",
    "        \n",
    "        if found_separate or found_merged:\n",
    "            found_complex_blocks.append(complex_block_name)\n",
    "    \n",
    "    if len(found_complex_blocks) != 7:\n",
    "        errors.append(f\"Incomplete complex block coverage: {len(found_complex_blocks)}/7 complex blocks found (ALL 7 REQUIRED)\")\n",
    "    \n",
    "    # Check 6: At least 3 paragraphs must have 2 different complex blocks (handle both formats)\n",
    "    paragraphs_with_multiple_blocks = 0\n",
    "    for paragraph in paragraphs:\n",
    "        # Count unique complex blocks in this paragraph (both separate and merged formats)\n",
    "        blocks_in_paragraph = set()\n",
    "        for complex_block_name in required_complex_blocks:\n",
    "            short_name = complex_block_name.replace(\" \", \"_\").replace(\",\", \"\").replace(\"-\", \"_\")\n",
    "            \n",
    "            # Check separate format #block#\n",
    "            if f\"#{complex_block_name}#\" in paragraph:\n",
    "                blocks_in_paragraph.add(complex_block_name)\n",
    "            \n",
    "            # Check merged format [BLOCK#complex]\n",
    "            elif (f\"#{complex_block_name}\" in paragraph or \n",
    "                  f\"#{short_name}\" in paragraph or\n",
    "                  complex_block_name.replace(\" \", \"_\") in paragraph):\n",
    "                blocks_in_paragraph.add(complex_block_name)\n",
    "        \n",
    "        # Check if this paragraph has 2 or more different complex blocks\n",
    "        if len(blocks_in_paragraph) >= 2:\n",
    "            paragraphs_with_multiple_blocks += 1\n",
    "    \n",
    "    if paragraphs_with_multiple_blocks < 3:\n",
    "        errors.append(f\"Insufficient paragraph complexity: only {paragraphs_with_multiple_blocks} paragraphs have 2+ complex blocks (need at least 3)\")\n",
    "    \n",
    "    is_valid = len(errors) == 0\n",
    "    return is_valid, errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e963fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complex_block(block_output, context=None):\n",
    "    \"\"\"\n",
    "    Simplified: Add complex block identifiers from complex_block.json to existing building blocks.\n",
    "    Format: #Block Name# (short explanation)\n",
    "    Uses definitions and examples from the JSON file for accurate implementation.\n",
    "    \"\"\"\n",
    "    max_iterations = 5\n",
    "    iteration = 1\n",
    "    feedback_history = []\n",
    "    \n",
    "    # Load complex blocks from JSON with definitions and examples\n",
    "    with open(\"./complex_block.json\", \"r\") as f:\n",
    "        complex_blocks = json.load(f)\n",
    "    \n",
    "    # Create detailed information about each complex block\n",
    "    complex_block_info = \"\"\n",
    "    for block_name, block_data in complex_blocks.items():\n",
    "        complex_block_info += f\"\\n- {block_name}:\\n\"\n",
    "        complex_block_info += f\"  Definition: {block_data['Definition']}\\n\"\n",
    "        complex_block_info += f\"  Examples: {'; '.join(block_data['Examples'])}\\n\"\n",
    "    \n",
    "    system_prompt = f\"\"\"You are a system prompt processor that adds complex block identifiers to building block structures using MIXED FORMAT.\n",
    "\n",
    "Your task:\n",
    "1. Keep the existing paragraph structure EXACTLY as is\n",
    "2. Add ALL 7 complex block identifiers based on their definitions - EVERY SINGLE ONE MUST BE INCLUDED\n",
    "3. Use MIXED FORMAT: 30-40% merged format, 60-70% separate format\n",
    "4. Use the provided definitions and examples to ensure correct application\n",
    "\n",
    "MIXED FORMAT REQUIREMENTS:\n",
    "- MERGED FORMAT (30-40% of building blocks): [BUILDING_BLOCK#Complex_Block_Name]\n",
    "- SEPARATE FORMAT (60-70% of building blocks): [BUILDING_BLOCK] #Complex Block Name# (explanation)\n",
    "\n",
    "CRITICAL REQUIREMENT: ALL 7 complex blocks must be included in the output. No exceptions.\n",
    "\n",
    "Available complex blocks with definitions and examples (ALL MUST BE USED):\n",
    "{complex_block_info}\n",
    "\n",
    "Instructions:\n",
    "- Keep EXACT same paragraph structure and content\n",
    "- Use MIXED FORMAT: Some blocks merged [BLOCK#complex], others separate [BLOCK] #complex#\n",
    "- Aim for 30-40% of building blocks to use merged format [BUILDING_BLOCK#Complex_Name]\n",
    "- Remaining 60-70% use separate format [BUILDING_BLOCK] #Complex Name# (brief explanation)\n",
    "- MANDATORY: Include ALL 7 complex blocks - distribute them across the paragraphs appropriately\n",
    "- MANDATORY: At least 3 paragraphs must have 2 different complex blocks\n",
    "- Maintain all original content including (parenthetical content)\n",
    "\n",
    "EXAMPLES:\n",
    "- Merged: [BACKGROUND_INFORMATION#Define_Personality_and_Tone] (explanation)\n",
    "- Separate: [TONAL_CONTROL] #Guide Tool Use and Response Formatting# (control formatting) \n",
    "- Mixed paragraph: [CONTEXT_INFORMATION#Provide_Context_Information] [USER_PREFERENCES] #Set Clear Guardrails# (safety)\n",
    "\n",
    "VALIDATION: The output will be checked to ensure:\n",
    "- ALL 7 complex blocks are present\n",
    "- At least 3 paragraphs have 2 different complex blocks\n",
    "- Mixed format used (30-40% merged, 60-70% separate)\n",
    "Missing any requirement will cause validation failure.\n",
    "\n",
    "Process the building block structure now and ensure ALL 7 complex blocks are included with proper mixed format distribution.\"\"\"\n",
    "    \n",
    "    structured_model_config = ModelConfig(model_id=\"gpt-5\", temperature=0.7, max_tokens=2500)\n",
    "    structured_model = OpenAIChat(structured_model_config)\n",
    "    \n",
    "    while iteration <= max_iterations:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ADDING COMPLEX BLOCKS - Iteration {iteration}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        user_message = f\"\"\"\n",
    "Add relevant complex block identifiers to this building block structure:\n",
    "\n",
    "{block_output}\n",
    "\n",
    "Context: {context if context else \"General use\"}\n",
    "\"\"\"\n",
    "        \n",
    "        if feedback_history:\n",
    "            user_message += \"\\n\\nPrevious feedback to incorporate:\\n\" + \"\\n\".join(feedback_history)\n",
    "        \n",
    "        # Automatic retry with validation\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        valid_response = None\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_message.strip()}\n",
    "                ]\n",
    "                \n",
    "                print(f\"Adding complex blocks... (Attempt {retry_count + 1}/{max_retries})\")\n",
    "                response = structured_model.generate(messages)\n",
    "                \n",
    "                validation_passed, validation_errors = validate_requirements_response(response, structured_model)\n",
    "                \n",
    "                if validation_passed:\n",
    "                    print(\"âœ… Validation passed!\")\n",
    "                    print(\"Generated Requirements Structure:\")\n",
    "                    print(\"-\" * 40)\n",
    "                    print(response)\n",
    "                    print(\"-\" * 40)\n",
    "                    valid_response = response\n",
    "                    break\n",
    "                else:\n",
    "                    retry_count += 1\n",
    "                    print(f\"âŒ Validation failed (Attempt {retry_count}/{max_retries}):\")\n",
    "                    for error in validation_errors:\n",
    "                        print(f\"  - {error}\")\n",
    "                    \n",
    "                    if retry_count < max_retries:\n",
    "                        validation_feedback = \"Previous attempt failed validation:\\n\" + \"\\n\".join(validation_errors)\n",
    "                        user_message += f\"\\n\\nIMPORTANT - Fix these issues:\\n{validation_feedback}\"\n",
    "                    else:\n",
    "                        print(\"Max retries reached. Using last response despite validation issues.\")\n",
    "                        valid_response = response\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"Error (Attempt {retry_count}/{max_retries}): {str(e)}\")\n",
    "                if retry_count >= max_retries:\n",
    "                    valid_response = f\"Error: {str(e)}\"\n",
    "        \n",
    "        response = valid_response\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        \n",
    "        feedback = input(\"\\nProvide feedback (or type 'good' to finish, 'stop' to end): \").strip()\n",
    "        \n",
    "        if feedback == \"\":\n",
    "            return response\n",
    "        \n",
    "        if feedback.lower() in ['done', 'good', 'good!', 'looks good', 'perfect']:\n",
    "            print(\"\\nGreat! Requirements structure processing completed successfully.\")\n",
    "            return response\n",
    "            \n",
    "        if feedback.lower() in ['stop', 'quit', 'exit']:\n",
    "            return response\n",
    "        \n",
    "        if feedback:\n",
    "            feedback_history.append(f\"Iteration {iteration}: {feedback}\")\n",
    "            print(f\"Feedback recorded: {feedback}\")\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bfff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_content_from_complex_block_improved(complex_block, context=None):\n",
    "    \"\"\"\n",
    "    Generate actual system prompt content from complex block identifiers.\n",
    "    Uses both structure.json and complex_block.json for accurate content generation.\n",
    "    \n",
    "    Args:\n",
    "        complex_block: String containing complex block identifiers like #Block Name# \n",
    "        context: Optional context information to tailor the content\n",
    "    \n",
    "    Returns:\n",
    "        String: The populated system prompt with actual content\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load both structure definitions and complex block definitions\n",
    "    with open(\"./structure.json\", \"r\") as f:\n",
    "        requirement_structure_explain = json.load(f)['requirements']\n",
    "    \n",
    "    with open(\"./complex_block.json\", \"r\") as f:\n",
    "        complex_blocks = json.load(f)\n",
    "    \n",
    "    max_iterations = 5\n",
    "    iteration = 1\n",
    "    feedback_history = []\n",
    "    \n",
    "    # Extract context information for tailoring\n",
    "    context_info = \"\"\n",
    "    if context:\n",
    "        context_info = f\"Context: {context}\"\n",
    "    \n",
    "    # Create detailed complex block information\n",
    "    complex_block_info = \"\"\n",
    "    for block_name, block_data in complex_blocks.items():\n",
    "        complex_block_info += f\"\\n- {block_name}:\\n\"\n",
    "        complex_block_info += f\"  Definition: {block_data['Definition']}\\n\"\n",
    "        complex_block_info += f\"  Examples: {'; '.join(block_data['Examples'])}\\n\"\n",
    "    \n",
    "    # System prompt for generating actual content\n",
    "    system_prompt = f\"\"\"You are a system prompt content generator that converts complex block identifiers into actual system prompt content.\n",
    "\n",
    "Your task:\n",
    "1. Replace each complex block identifier (like #Block Name#) with actual system prompt content\n",
    "2. Use the provided definitions and examples from complex_block.json for each block\n",
    "3. Use structure.json for additional technical details if needed\n",
    "4. Tailor the content to be suitable for the given context\n",
    "5. Follow the format: \"actual_content [complexity_block]\" where complexity_block is the original identifier\n",
    "6. Generate natural, coherent system prompt content that flows well\n",
    "\n",
    "Available complex block definitions and examples:\n",
    "{complex_block_info}\n",
    "\n",
    "{context_info}\n",
    "\n",
    "Instructions:\n",
    "- Keep the exact same paragraph structure as the input\n",
    "- Replace each #identifier# with appropriate content based on its definition followed by [complexity_block]\n",
    "- Content should be natural system prompt instructions that implement the definition\n",
    "- Use the examples provided to understand how to apply each block correctly\n",
    "- Tailor examples and language to the provided context\n",
    "- Maintain coherent flow between sentences\n",
    "- Each piece of content should be actionable system prompt instructions\n",
    "\n",
    "Process the provided complex block structure now.\"\"\"\n",
    "    \n",
    "    # Create model for content generation\n",
    "    content_model_config = ModelConfig(model_id=\"gpt-4\", temperature=0.7, max_tokens=3000)\n",
    "    content_model = OpenAIChat(content_model_config)\n",
    "    \n",
    "    while iteration <= max_iterations:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"POPULATING CONTENT FROM COMPLEX BLOCK - Iteration {iteration}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Build user message with the complex block\n",
    "        user_message = f\"\"\"\n",
    "Please convert each complex block identifier into actual system prompt content while maintaining the exact same structure.\n",
    "\n",
    "Here is the complex block structure to populate:\n",
    "\n",
    "{complex_block}\n",
    "\n",
    "Remember to:\n",
    "- Replace each #identifier# with relevant content based on its definition + [complexity_block]\n",
    "- Keep all parenthetical content unchanged\n",
    "- Use the definitions and examples from complex_block.json to ensure accuracy\n",
    "- Tailor content to the provided context\n",
    "- Make content actionable and clear\n",
    "- Maintain natural flow\n",
    "\"\"\"\n",
    "        \n",
    "        # Add feedback history if exists\n",
    "        if feedback_history:\n",
    "            user_message += \"\\n\\nPrevious feedback to incorporate:\\n\" + \"\\n\".join(feedback_history)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message.strip()}\n",
    "            ]\n",
    "            \n",
    "            print(\"Populating complex block with actual content...\")\n",
    "            response = content_model.generate(messages)\n",
    "            \n",
    "            print(\"Generated System Prompt Content:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(response)\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error populating content: {str(e)}\")\n",
    "            response = f\"Error: {str(e)}\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        \n",
    "        # Get user feedback\n",
    "        feedback = input(\"\\nProvide feedback (or type 'good' to finish, 'stop' to end): \").strip()\n",
    "        \n",
    "        if feedback == \"\":\n",
    "            print(\"No feedback provided. Content population complete...\")\n",
    "            return response\n",
    "        \n",
    "        if feedback.lower() in ['done', 'good', 'good!', 'looks good', 'perfect']:\n",
    "            print(\"\\nGreat! Content population completed successfully.\")\n",
    "            return response\n",
    "            \n",
    "        if feedback.lower() in ['stop', 'quit', 'exit']:\n",
    "            print(\"\\nStopping content population.\")\n",
    "            return response\n",
    "        \n",
    "        if feedback:\n",
    "            feedback_history.append(f\"Iteration {iteration}: {feedback}\")\n",
    "            print(f\"Feedback recorded: {feedback}\")\n",
    "            print(\"Re-generating with your feedback...\")\n",
    "        else:\n",
    "            print(\"No feedback provided. Re-generating...\")\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    print(f\"\\nReached maximum iterations ({max_iterations}). Returning final result.\")\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_complex_block_coverage(response):\n",
    "    \"\"\"\n",
    "    Analyze and display complex block coverage in the response.\n",
    "    Shows which blocks are included and which are missing.\n",
    "    \"\"\"\n",
    "    # Load complex blocks from JSON\n",
    "    with open(\"./complex_block.json\", \"r\") as f:\n",
    "        complex_blocks = json.load(f)\n",
    "    \n",
    "    all_complex_blocks = list(complex_blocks.keys())\n",
    "    found_blocks = []\n",
    "    missing_blocks = []\n",
    "    \n",
    "    print(\"=== COMPLEX BLOCK COVERAGE ANALYSIS ===\")\n",
    "    print(f\"Total available complex blocks: {len(all_complex_blocks)}\")\n",
    "    print()\n",
    "    \n",
    "    for block_name in all_complex_blocks:\n",
    "        if f\"#{block_name}#\" in response:\n",
    "            found_blocks.append(block_name)\n",
    "            print(f\"âœ… FOUND: {block_name}\")\n",
    "        else:\n",
    "            missing_blocks.append(block_name)\n",
    "            print(f\"âŒ MISSING: {block_name}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Coverage Summary: {len(found_blocks)}/{len(all_complex_blocks)} complex blocks included\")\n",
    "    print(f\"Coverage Percentage: {(len(found_blocks)/len(all_complex_blocks))*100:.1f}%\")\n",
    "    \n",
    "    if missing_blocks:\n",
    "        print(f\"\\nMissing blocks ({len(missing_blocks)}):\")\n",
    "        for block in missing_blocks:\n",
    "            print(f\"  - {block}\")\n",
    "            print(f\"    Definition: {complex_blocks[block]['Definition'][:100]}...\")\n",
    "    \n",
    "    return len(found_blocks), len(missing_blocks)\n",
    "\n",
    "# Test function to show all available complex blocks\n",
    "def show_all_complex_blocks():\n",
    "    \"\"\"Display all available complex blocks with their definitions.\"\"\"\n",
    "    with open(\"./complex_block.json\", \"r\") as f:\n",
    "        complex_blocks = json.load(f)\n",
    "    \n",
    "    print(\"=== ALL AVAILABLE COMPLEX BLOCKS ===\")\n",
    "    for i, (block_name, block_data) in enumerate(complex_blocks.items(), 1):\n",
    "        print(f\"{i}. {block_name}\")\n",
    "        print(f\"   Definition: {block_data['Definition']}\")\n",
    "        print(f\"   Examples: {len(block_data['Examples'])} provided\")\n",
    "        print()\n",
    "\n",
    "# Show all available blocks\n",
    "show_all_complex_blocks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a58cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_content_from_complex_block(complex_block, context=None):\n",
    "    \"\"\"\n",
    "    Generate actual system prompt content from complex block identifiers.\n",
    "    \n",
    "    Args:\n",
    "        complex_block: String containing complex block identifiers like [background_information@dynamic_behavior_scaling@complexity_assessment]\n",
    "        context: Optional context information to tailor the content\n",
    "    \n",
    "    Returns:\n",
    "        String: The populated system prompt with actual content\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"./structure.json\", \"r\") as f:\n",
    "        requirement_structure_explain = json.load(f)['requirements']\n",
    "    \n",
    "    max_iterations = 5\n",
    "    iteration = 1\n",
    "    feedback_history = []\n",
    "    \n",
    "    # Extract context information for tailoring\n",
    "    context_info = \"\"\n",
    "    if context:\n",
    "        context_info = f\"Context: {context}\"\n",
    "    \n",
    "    # System prompt for generating actual content\n",
    "    system_prompt = f\"\"\"You are a system prompt content generator that converts complex block identifiers into actual system prompt content.\n",
    "\n",
    "Your task:\n",
    "1. Replace each complex block identifier (like [background_information@dynamic_behavior_scaling@complexity_assessment]) with actual system prompt content\n",
    "2. Use the provided explanations and examples from the requirement structure\n",
    "3. Tailor the content to be suitable for the given context\n",
    "4. Follow the format: \"actual_content [complexity_block]\" where complexity_block is the original identifier\n",
    "5. Generate natural, coherent system prompt content that flows well\n",
    "\n",
    "Available requirement explanations and examples:\n",
    "{json.dumps(requirement_structure_explain, indent=2)}\n",
    "\n",
    "{context_info}\n",
    "\n",
    "Instructions:\n",
    "- Keep the exact same paragraph structure as the input\n",
    "- Replace each [identifier] with appropriate content followed by [complexity_block]\n",
    "- Content should be natural system prompt instructions\n",
    "- Tailor examples and language to the provided context\n",
    "- Maintain coherent flow between sentences\n",
    "- Each piece of content should be actionable system prompt instructions\n",
    "\n",
    "Process the provided complex block structure now.\"\"\"\n",
    "    \n",
    "    # Create model for content generation\n",
    "    content_model_config = ModelConfig(model_id=\"gpt-5\", temperature=0.7, max_tokens=3000)\n",
    "    content_model = OpenAIChat(content_model_config)\n",
    "    \n",
    "    while iteration <= max_iterations:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"POPULATING CONTENT FROM COMPLEX BLOCK - Iteration {iteration}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Build user message with the complex block\n",
    "        user_message = f\"\"\"\n",
    "Please convert each complex block identifier into actual system prompt content while maintaining the exact same structure.\n",
    "\n",
    "Here is the complex block structure to populate:\n",
    "\n",
    "{complex_block}\n",
    "\n",
    "Remember to:\n",
    "- Replace each [identifier] with relevant content + [complexity_block]\n",
    "- Keep all parenthetical content unchanged\n",
    "- Tailor content to the context\n",
    "- Make content actionable and clear\n",
    "- Maintain natural flow\n",
    "\"\"\"\n",
    "        \n",
    "        # Add feedback history if exists\n",
    "        if feedback_history:\n",
    "            user_message += \"\\n\\nPrevious feedback to incorporate:\\n\" + \"\\n\".join(feedback_history)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message.strip()}\n",
    "            ]\n",
    "            \n",
    "            print(\"Populating complex block with actual content...\")\n",
    "            response = content_model.generate(messages)\n",
    "            \n",
    "            print(\"Generated System Prompt Content:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(response)\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error populating content: {str(e)}\")\n",
    "            response = f\"Error: {str(e)}\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        \n",
    "        # Get user feedback\n",
    "        feedback = input(\"\\nProvide feedback (or type 'good' to finish, 'stop' to end): \").strip()\n",
    "        \n",
    "        if feedback == \"\":\n",
    "            print(\"No feedback provided. Content population complete...\")\n",
    "            return response\n",
    "        \n",
    "        if feedback.lower() in ['done', 'good', 'good!', 'looks good', 'perfect']:\n",
    "            print(\"\\nGreat! Content population completed successfully.\")\n",
    "            return response\n",
    "            \n",
    "        if feedback.lower() in ['stop', 'quit', 'exit']:\n",
    "            print(\"\\nStopping content population.\")\n",
    "            return response\n",
    "        \n",
    "        if feedback:\n",
    "            feedback_history.append(f\"Iteration {iteration}: {feedback}\")\n",
    "            print(f\"Feedback recorded: {feedback}\")\n",
    "            print(\"Re-generating with your feedback...\")\n",
    "        else:\n",
    "            print(\"No feedback provided. Re-generating...\")\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    print(f\"\\nReached maximum iterations ({max_iterations}). Returning final result.\")\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28715ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_system_info(complex_structure, context, system_settings):\n",
    "    \"\"\"\n",
    "    Add 2-5 pieces of system setting information to the FIRST CONTEXT_INFORMATION block.\n",
    "    \n",
    "    Args:\n",
    "        complex_structure: String containing the complex structure (JSON format as string)\n",
    "        context: String containing user context information\n",
    "        system_settings: String containing system-specific settings and configurations\n",
    "    \n",
    "    Returns:\n",
    "        String: Enhanced complex structure with system info added to first CONTEXT_INFORMATION block\n",
    "    \"\"\"\n",
    "    \n",
    "    max_iterations = 3\n",
    "    iteration = 1\n",
    "    feedback_history = []\n",
    "    \n",
    "    system_prompt = \"\"\"You are a system prompt enhancer that adds system setting information to the FIRST CONTEXT_INFORMATION block.\n",
    "\n",
    "Your task:\n",
    "1. Analyze the provided context and system settings to understand the user's environment\n",
    "2. Generate 2-5 pieces of relevant system setting information\n",
    "3. Add this information ONLY to the FIRST CONTEXT_INFORMATION block you find in the structure\n",
    "4. Leave all other blocks completely unchanged\n",
    "\n",
    "Requirements:\n",
    "- Add 2-5 pieces of system setting information to the FIRST CONTEXT_INFORMATION block only\n",
    "- Generate system info dynamically based on context and settings\n",
    "- Maintain the exact structure and format of the input\n",
    "- Make additions feel natural and integrated\n",
    "- Do NOT modify any other blocks\n",
    "\n",
    "Instructions:\n",
    "- Find the FIRST CONTEXT_INFORMATION block in the structure\n",
    "- Add 2-5 pieces of relevant system setting information to that block only\n",
    "- Generate system info that would be helpful for the specific context and settings\n",
    "- Format: Add as natural extensions within the first CONTEXT_INFORMATION block\n",
    "- Example: \"[CONTEXT_INFORMATION] existing content (system: mobile device, storage: 64GB, network: WiFi)\"\n",
    "\n",
    "CRITICAL: Enhance ONLY the FIRST CONTEXT_INFORMATION block. Leave all others unchanged.\n",
    "\n",
    "Focus on adding system setting information that's specifically relevant to the context.\"\"\"\n",
    "    \n",
    "    model_config = ModelConfig(model_id=\"gpt-4\", temperature=0.7, max_tokens=3000)\n",
    "    model = OpenAIChat(model_config)\n",
    "    \n",
    "    while iteration <= max_iterations:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ADDING SYSTEM INFO TO FIRST CONTEXT_INFORMATION BLOCK - Iteration {iteration}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        user_message = f\"\"\"\n",
    "Add 2-5 pieces of system setting information to the FIRST CONTEXT_INFORMATION block:\n",
    "\n",
    "COMPLEX STRUCTURE:\n",
    "{complex_structure}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "SYSTEM SETTINGS:\n",
    "{system_settings}\n",
    "\n",
    "Requirements:\n",
    "- Add 2-5 pieces of system setting information to the FIRST CONTEXT_INFORMATION block only\n",
    "- Generate relevant system info based on context and system settings\n",
    "- Maintain exact structure and format\n",
    "- Leave all other blocks unchanged\n",
    "\"\"\"\n",
    "        \n",
    "        if feedback_history:\n",
    "            user_message += \"\\n\\nPrevious feedback to incorporate:\\n\" + \"\\n\".join(feedback_history)\n",
    "        \n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message.strip()}\n",
    "            ]\n",
    "            \n",
    "            print(\"Adding system setting information to first CONTEXT_INFORMATION block...\")\n",
    "            response = model.generate(messages)\n",
    "            \n",
    "            print(\"Enhanced Complex Structure:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(response)\n",
    "            print(\"-\" * 40)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding system info: {str(e)}\")\n",
    "            response = f\"Error: {str(e)}\"\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        \n",
    "        # Get user feedback\n",
    "        feedback = input(\"\\nProvide feedback (or type 'good' to finish, 'stop' to end): \").strip()\n",
    "        \n",
    "        if feedback == \"\":\n",
    "            print(\"No feedback provided. System info addition complete...\")\n",
    "            return response\n",
    "        \n",
    "        if feedback.lower() in ['done', 'good', 'good!', 'looks good', 'perfect']:\n",
    "            print(\"\\nGreat! System info addition completed successfully.\")\n",
    "            return response\n",
    "            \n",
    "        if feedback.lower() in ['stop', 'quit', 'exit']:\n",
    "            print(\"\\nStopping system info addition.\")\n",
    "            return response\n",
    "        \n",
    "        if feedback:\n",
    "            feedback_history.append(f\"Iteration {iteration}: {feedback}\")\n",
    "            print(f\"Feedback recorded: {feedback}\")\n",
    "            print(\"Re-generating with your feedback...\")\n",
    "        else:\n",
    "            print(\"No feedback provided. Re-generating...\")\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    print(f\"\\nReached maximum iterations ({max_iterations}). Returning final result.\")\n",
    "    return response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
